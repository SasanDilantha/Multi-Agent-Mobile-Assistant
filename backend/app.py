# backend/app.py

from flask import Flask, request, jsonify
from flask_cors import CORS
from langchain_community.llms import Ollama
from langchain.agents import AgentExecutor, create_react_agent
from langchain_core.prompts import PromptTemplate
from langchain_core.tools import Tool
from langchain_core.messages import HumanMessage, AIMessage
import os

app = Flask(__name__)
CORS(app) # Enable CORS for all routes

# --- Initialize Ollama LLM ---
# Connect to Ollama using its service name in Docker Compose
# The port is 11434 as defined by Ollama.
# The host 'ollama-server' refers to the service name in docker-compose.yml
OLLAMA_HOST = os.getenv("OLLAMA_HOST", "http://ollama-server:11434")
llm = Ollama(model="gemma:2b", base_url=OLLAMA_HOST)

# --- Define Tools (simplified for this tutorial) ---
def planner_tool(task: str) -> str:
    """
    A tool for the Planner Agent to generate a high-level plan.
    It uses the LLM to create a plan based on the task.
    """
    print(f"Planner Tool received task: {task}")
    prompt = f"As a helpful planner, generate a concise, step-by-step plan to address the following request: '{task}'. Focus on logical steps. Example: '1. Understand the query. 2. Gather information. 3. Formulate response.'"
    plan = llm.invoke(prompt)
    print(f"Generated Plan: {plan}")
    return plan

def executor_tool(plan: str) -> str:
    """
    A tool for the Executor Agent to execute a given plan.
    It uses the LLM to generate a detailed response based on the plan.
    """
    print(f"Executor Tool received plan: {plan}")
    prompt = f"As an executor, based on the following plan, provide a detailed and helpful response to the original request. Plan: '{plan}'"
    response = llm.invoke(prompt)
    print(f"Generated Response: {response}")
    return response

# Create LangChain Tools
tools = [
    Tool(
        name="Planner",
        func=planner_tool,
        description="Useful for creating a step-by-step plan to address a user's request."
    ),
    Tool(
        name="Executor",
        func=executor_tool,
        description="Useful for executing a plan and generating a detailed response."
    ),
]

# --- Define Agent Prompts ---
# Planner Agent Prompt
planner_prompt_template = PromptTemplate.from_template("""
You are a helpful AI assistant specialized in planning.
You will be given a user's request, and your goal is to break it down into a clear, actionable plan.
Use the 'Planner' tool to generate this plan.

TOOLS:
{tools}

HUMAN_MESSAGE: {input}
{agent_scratchpad}
""")

# Executor Agent Prompt
executor_prompt_template = PromptTemplate.from_template("""
You are a helpful AI assistant specialized in executing plans.
You will be given a plan generated by a planner, and your goal is to provide a comprehensive response based on that plan.
Use the 'Executor' tool to generate the final response.

TOOLS:
{tools}

HUMAN_MESSAGE: {input}
{agent_scratchpad}
""")

# --- Create Agents ---
planner_agent = create_react_agent(llm, tools, planner_prompt_template)
planner_executor = AgentExecutor(agent=planner_agent, tools=tools, verbose=True, handle_parsing_errors=True)

executor_agent = create_react_agent(llm, tools, executor_prompt_template)
executor_executor = AgentExecutor(agent=executor_agent, tools=tools, verbose=True, handle_parsing_errors=True)

# --- Multi-Agent Orchestration (Simplified) ---
@app.route('/ask', methods=['POST'])
def ask_agents():
    user_query = request.json.get('query')
    if not user_query:
        return jsonify({"error": "No query provided"}), 400

    try:
        # Step 1: Planner Agent generates a plan
        print(f"\n--- Initiating Planner for: '{user_query}' ---")
        planner_result = planner_executor.invoke({"input": user_query})
        plan = planner_result.get("output", "No plan generated.")
        print(f"Planner Output (Plan): {plan}")

        # Step 2: Executor Agent executes the plan
        print(f"\n--- Initiating Executor with Plan: '{plan}' ---")
        executor_result = executor_executor.invoke({"input": plan})
        final_response = executor_result.get("output", "No response generated.")
        print(f"Executor Output (Final Response): {final_response}")

        return jsonify({"response": final_response})

    except Exception as e:
        print(f"Error during agent execution: {e}")
        return jsonify({"error": str(e)}), 500

@app.route('/')
def home():
    return "Multi-Agent Mobile Backend is running!"

if __name__ == '__main__':
    # Flask's default debug mode might not work well with Docker.
    # For production, disable debug. For development, it's fine.
    app.run(host='0.0.0.0', port=5000, debug=True)
